from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, TrainingArguments, Trainer
from torch.utils.data import Dataset
import json

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2",
    bos_token='</s>', eos_token='</s>', unk_token='<unk>',
    pad_token='<pad>', mask_token='<mask>') 
model = GPT2LMHeadModel.from_pretrained("skt/kogpt2-base-v2")

# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class MyDataset(Dataset):
    def __init__(self, file_path, tokenizer):
        self.tokenizer = tokenizer
        with open(file_path, 'r', encoding='utf-8') as f:
            self.data = json.loads(f.read())

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        question, context = item['input'].split(' context: ')
        question = question.replace('question: ', '')
        print(context)
        inputs = self.tokenizer(question, context, truncation=True, padding='max_length', max_length=1024, return_tensors='pt')
        target = self.tokenizer(item['target'], truncation=True, padding='max_length', max_length=128, return_tensors='pt')['input_ids']
        inputs['labels'] = target
        return inputs


# Trainê³¼ validation ë°ì´í„°ì…‹ ë¡œë“œ
train_dataset = MyDataset('/home/t23108/svr/JH_PRACTICE/AI/chatBot/trainData.json', tokenizer)
# val_dataset = MyDataset('path_to_val.json', tokenizer)

# í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
     report_to = [],                  # empty list to avoid reporting to any service
)

# Trainer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
trainer = Trainer(
    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    # eval_dataset=val_dataset             # evaluation dataset
)

# í•™ìŠµ ì‹œì‘
trainer.train()

# ë‹¤ë¥¸ ê²½ë¡œì— ëª¨ë¸ ì €ì¥
trainer.save_model()
